{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Capstone Progress\n",
    "\n",
    "    Identify: Articulate Problem Statement/Specific goals & success criteria\n",
    "    Identify: Outline proposed methods & models\n",
    "    Parse: Identify risks & assumptions\n",
    "    Parse: Your Database solution\n",
    "    Parse: Your data source and some data challenges\n",
    "    Parse: Interesting data structures\n",
    "    Mine: Perform & summarize EDA (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify: Articulate Problem Statement/Specific goals & success criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am extending Project 6.  \n",
    "\n",
    "First, I need to improve on the piplines from project 6.  There are a number of tweaks to improve behavior of the piplines.\n",
    "\n",
    "Second, Improving the predictive categorization is major step.  Starting with Cosine Similarity, the prediction on the train/test split is nearly 90% accuracy.  XGboost performs about 92% performance.  A wider range of classsifiers need be explored and optimized to improve accuracy and and generalization.\n",
    "\n",
    "The ultimate goal is to accurately predict categorization of new documents, and integrate new topics.\n",
    "\n",
    "The final implemtation is intended to run in a containerized service, with queues utilized to add new categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify: Outline proposed methods & models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods: \n",
    "- Utilize wikipedia api to acquire baseline of categories to be matched compared.\n",
    "- Use LSA to model categorizations. \n",
    "- Test calssification methods to quickly cateogrize new data.\n",
    "- Utilize Docker or other containers to make this Data aquisition, model generation and data storage.\n",
    "- Automate category addition when there is poor category fit.\n",
    "<div class=\"pagebreak\"></div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Parse: Identify risks & assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risks:\n",
    "- Replicating the work of half class/Project will not be distinct from others.  \n",
    "\n",
    "Assumption:\n",
    "- There is value in knowin LSA.  Is it a commidity level skill?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse: Your Database solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presently, I'm using postgres based on ease of use.  The main advantage is maintaining references for a many-to-many relationship and using that relationship to predict matches for multiple categories.\n",
    "\n",
    "Moving to a NoSQL soltution, may be very helpful when datasources other than wikipedia are included.  Also, the current index of categories is based on the the wikipedia catgory_id/page_id of the category.  This should be made independent of the datasource and another field can contain the metadata about sources.  Maybe as a JSON object.  Maybe this should be in Mongo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse: Your data source and some data challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data challenges: Cleaning the text to deal with unicode elements: the especially the punctuation elements, (em-dash, double dash).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse: Interesting data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structures are very simple.  Just lists, dicts, and tuples.  The utilization could  better and standardized. All the code will need to be refactored to utilze the data better.\n",
    "\n",
    "Also, much of the code could be abstracted further. to minimize redunancies.\n",
    "\n",
    "<div class=\"pagebreak\"></div>\n",
    "\n",
    "## Mine: Perform & summarize EDA (optional)\n",
    "\n",
    "EDA:  This is much more focused on the process than the results of the data.  The data is well formed, but it does contain some pesky artifacts.  There maybe utiliity/oportunities to include various markup langauges, like latex and markdown.\n",
    "\n",
    "The main group of 23 categories from Wikipedia shows a pretty broad distribution of content.  \n",
    "\n",
    "| category_name | count |\n",
    "|:---------------|:-------:|\n",
    "|sports cars| 593 |\n",
    "|Arcade games| 1644 |\n",
    "|desserts| 161 |\n",
    "|chemistry| 120 |\n",
    "|physics| 45 |\n",
    "|psychology| 288 |\n",
    "|cat breeds| 94 |\n",
    "|Earth sciences| 105 |\n",
    "|Sandwiches| 122 |\n",
    "|Breads| 127 |\n",
    "|Automotive technologies| 163 |\n",
    "|belief| 72 |\n",
    "|hygiene| 95 |\n",
    "|sports terminology| 178 |\n",
    "|shoes| 92 |\n",
    "|influenza| 40 |\n",
    "|Children| 6 |\n",
    "|Physical_quantities| 232 |\n",
    "|Submarine sandwich restaurants| 42 |\n",
    "|Association football trophies and awards| 58 |\n",
    "|Physical exercise|\t190 |\n",
    "|health care| 85 |\n",
    "|machine learning| 189 |\n",
    "\n",
    "There is a single category, Arcade Games, with much larger representation in the corpus than the average category(206 articles, on average).  This category does appear in the more frequently in the search function than other categories, in part due to it's size, and in part due to the breadth of topics in that category.  If this trend continutes with the addtion of more categories, the construction of the model will need to be addressed.\n",
    "<div class=\"pagebreak\"></div>\n",
    "After reducing the features of the SVD and a tSNE transformation, there do appear to be some  clusters.  Each color is a different category.\n",
    "\n",
    "![title](doc/tsne_of_SVD_40_comp.png)\n",
    "<div class=\"pagebreak\"></div>\n",
    " ### Current todo list.  It is evolving and prone to being fanciful.  Some of it is completed.\n",
    "\n",
    "Problems with the solution provide by the teams.\n",
    "1. Values passed from module to module were not perfectly aligned.  The modules can be brought refactored to be more efficient.\n",
    "\n",
    "2. The downloads could not effectively handle categories that reached the maximum number of return from the api.  \n",
    "\n",
    "3. The model is seems very large for the amount of information included. For example, when the raw text was 29MB, the model is 410 MB.\n",
    "\n",
    "4. Using a Postgres database, single or double quotes (‘ or ` or “) can cause problems.  Moving to a noSQL database may improve this situation.\n",
    "\n",
    "5. A progress bar would be nice to have for some of the longer processes.\n",
    "\n",
    "6. Lemmatize!!\n",
    "\n",
    "7. The stop words being used need to be examined, and how the model applies the cut off for features.  Is the dropped features relevant.\n",
    "\n",
    "8. Category prediction is really poor.  Matching on the corpus of a category is not being effective.  t-SNE might be an option or average over the kNN for a many neighbors.\n",
    "\n",
    "9. Rewrite Search.py at work from Command line\n",
    "\n",
    "10. Recurse over subcategories.  Make pages members of parent and sub-categories.\n",
    "\n",
    "11. Document the words for vectorization.\n",
    "\n",
    "12. API\n",
    "\n",
    "13. Web interface.\n",
    "\n",
    "14. Generalize to unlabeled categories?\n",
    "\n",
    "15. How to load the vectorizer from redis?\n",
    "\n",
    "16. Update the vectorizer with diffs/rsync in redis? Or just overwrite? Or memcache?\n",
    "\n",
    "17. Check times, especially downloading, encoding and database reads.\n",
    "\n",
    "18. Suppress outputs/add verbose modes.\n",
    "\n",
    "19. Refactor database module. abstract or remove the error handling.\n",
    "\n",
    "20. Build docker then docker swarm. \n",
    "\n",
    "21. Add rq-montioring.and rq-dashboard.\n",
    "\n",
    "22. Docker compose = make the system\n",
    "\n",
    "23. Docker swarm, many machines feeding a main docker client\n",
    "\n",
    "24. Check if search model works better with string or list.\n",
    "\n",
    "25. Grid search models, SVD components plot vs accuracy of test set to be categorized. Maybe use subcategories as test data.\n",
    "\n",
    "26. Plot model creation time vs. \n",
    "\n",
    "27. SGD for categorization, or Xgboost.\n",
    "\n",
    "28. Test 2-grams or 3-grams.\n",
    "\n",
    "29. Put progress bar on build_model.py\n",
    "\n",
    "30. Add hash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
